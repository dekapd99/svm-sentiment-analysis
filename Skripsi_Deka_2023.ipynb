{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84ftMs7k6feV"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "!pip install Sastrawi\n",
        "!pip install swifter\n",
        "import csv\n",
        "import sklearn\n",
        "import swifter\n",
        "import torch\n",
        "import ast\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.utils import resample\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.datasets import make_classification\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LOAD DATA"
      ],
      "metadata": {
        "id": "YvdXIE5W7kIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Get dataset url from github repository\n",
        "url = \"https://raw.githubusercontent.com/dekapd99/svm-sentiment-analysis/master/dataset.csv\"\n",
        "\n",
        "# Read the dataset with pandas\n",
        "try:\n",
        "    df = pd.read_csv(url, delimiter=',')\n",
        "except pd.errors.ParserError as e:\n",
        "    print(\"Error occurred while parsing the CSV file:\", e)\n",
        "\n",
        "df"
      ],
      "metadata": {
        "id": "GBsELGXc7jLE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Drop Unused Columns"
      ],
      "metadata": {
        "id": "UYejK9rh7vZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the unused Columns\n",
        "df_dropped = df.drop(['created_at', 'username', 'id', 'id_str', 'quote_count','reply_count',\n",
        "                      'retweet_count', 'favorite_count', 'geo', 'lang', 'conversation_id',\n",
        "                      'conversation_id_str', 'media_url_https', 'media_type', 'user_id_str'], axis=1)\n",
        "\n",
        "# Export the df_dropped DataFrame to a CSV file\n",
        "df_dropped.to_csv('df_dropped.csv', index=False)\n",
        "\n",
        "df_dropped"
      ],
      "metadata": {
        "id": "gJUx3mmG7sC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.DataFrame(df_dropped[[\"new_date\", \"tweet\"]])\n",
        "data"
      ],
      "metadata": {
        "id": "xiPFd4VZ709o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TEXT PREPROCESSING"
      ],
      "metadata": {
        "id": "zXPxXE4-77HR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Case Folding"
      ],
      "metadata": {
        "id": "63rdsNsK8FLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Case Folding --> Convert Uppercase Tweet to Lowercase\n",
        "data[\"tweet\"] = data[\"tweet\"].str.lower()\n",
        "\n",
        "print(\"Case Folding Results: \\n\")\n",
        "print(\"================================ \\n\")\n",
        "data[\"tweet\"].head(5)"
      ],
      "metadata": {
        "id": "mrCS97JB78AP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.to_csv('df_case_folding.csv', index=False)\n",
        "data.head(5)"
      ],
      "metadata": {
        "id": "sq6ywNyH7947"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Cleaning"
      ],
      "metadata": {
        "id": "r8y5SvSp8BwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Cleaning --> Delete Mention Username\n",
        "def remove_pattern(input_txt, pattern):\n",
        "  r = re.findall(pattern, input_txt)\n",
        "  for i in r:\n",
        "    input_txt = re.sub(i, '', input_txt)\n",
        "  return input_txt\n",
        "\n",
        "data[\"tweet_mention_removed\"] = np.vectorize(remove_pattern)(data[\"tweet\"], \"@[\\w]*\")\n",
        "data"
      ],
      "metadata": {
        "id": "Y-JQ1_Jd8HI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing Retweet, Number, Punctuation, Emoji / Emotikon, Space, New Line, Hashtag, Link URL\n",
        "def remove(tweet):\n",
        "  tweet = re.sub(r'^rt[\\s]+', '', tweet)\n",
        "  tweet = re.sub(r'#', '', tweet)\n",
        "  tweet = re.sub(r'https\\S+', '', tweet)\n",
        "  tweet = re.sub(r'[0-9]+', '', tweet)\n",
        "  tweet = re.sub(r'\\_', \" \", tweet)\n",
        "  tweet = re.sub('\\s+', \" \", tweet)\n",
        "  tweet = re.sub(r\"\\W+\", \" \", tweet)\n",
        "  tweet = re.sub(r'\\\\n', \"\", tweet)\n",
        "  tweet = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", tweet)\n",
        "  tweet = emoji.replace_emoji(tweet, replace='')\n",
        "  tweet = re.sub(r':', '', tweet)\n",
        "  return tweet\n",
        "\n",
        "def remove_whitespace(tweet):\n",
        "  return ' '.join(tweet.split())\n",
        "\n",
        "data[\"clean_tweet\"] = data[\"tweet_mention_removed\"].apply(lambda x: remove(x))\n",
        "\n",
        "data.head(10)"
      ],
      "metadata": {
        "id": "ayrGEYlb8Eb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.to_csv('df_clean_tweet.csv', index=False)\n",
        "data.head(5)"
      ],
      "metadata": {
        "id": "lIU9rRNM8MJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Tokenization"
      ],
      "metadata": {
        "id": "EY9EYMGH8R0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get dataset url from github repository\n",
        "url = \"https://raw.githubusercontent.com/dekapd99/svm-sentiment-analysis/master/clean_tweet.csv\"\n",
        "\n",
        "# Read the dataset with pandas\n",
        "try:\n",
        "    data = pd.read_csv(url, delimiter=',')\n",
        "except pd.errors.ParserError as e:\n",
        "    print(\"Error occurred while parsing the CSV file:\", e)\n",
        "\n",
        "data.head(5)"
      ],
      "metadata": {
        "id": "cbnE0Omh8O3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download nltk resources (only needed once)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Convert clean_tweet values to strings\n",
        "data['clean_tweet'] = data['clean_tweet'].astype(str)\n",
        "\n",
        "# Tokenize the clean_tweet column\n",
        "data['tokens_tweet'] = data['clean_tweet'].apply(lambda x: word_tokenize(x))\n",
        "\n",
        "# Print the tokenized tweets\n",
        "print(data['tokens_tweet'])"
      ],
      "metadata": {
        "id": "C63XyT4s8RSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.to_csv('df_token_tweet.csv', index=False)\n",
        "data.head(5)"
      ],
      "metadata": {
        "id": "YBB2pbO38XUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Normalization"
      ],
      "metadata": {
        "id": "8GmzW4p_8lTX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalization\n",
        "normalizad_word = pd.read_excel(\"https://raw.githubusercontent.com/dekapd99/svm-sentiment-analysis/master/new_normalisasi.xlsx\")\n",
        "\n",
        "normalizad_word_dict = {}\n",
        "\n",
        "for index, row in normalizad_word.iterrows():\n",
        "    if row[0] not in normalizad_word_dict:\n",
        "        normalizad_word_dict[row[0]] = row[1]\n",
        "\n",
        "def normalized_term(document):\n",
        "    return [normalizad_word_dict[term] if term in normalizad_word_dict else term for term in document]\n",
        "\n",
        "data['normalized_tweet'] = data['tokens_tweet'].apply(normalized_term)\n",
        "\n",
        "data.head(5)"
      ],
      "metadata": {
        "id": "Gbhc8BqS8nO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.to_csv('df_normalized_tweet.csv', index=False)\n",
        "data.head(5)"
      ],
      "metadata": {
        "id": "G4adFdBH8pjL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Stopwords"
      ],
      "metadata": {
        "id": "0-C5VIn68tvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "factory = StopWordRemoverFactory()\n",
        "more_stopword = ['yg', 'dg', 'rt', 'dgn', 'ny', 'd', 'klo', 'bapak', 'tingkat', 'buat',\n",
        "                  'kalo', 'amp', 'biar', 'bikin', 'bilang','ibu', 'jadi', 'laku',\n",
        "                  'gak', 'ga', 'krn', 'nya', 'nih', 'sih', 'anda', 'terus', 'maksimal',\n",
        "                  'si', 'tau', 'tdk', 'tuh', 'utk', 'ya', 'ente', 'masyarakat',\n",
        "                  'jd', 'jgn', 'sdh', 'aja', 'n', 't', 'dan', 'sangat', 'moga', 'rakyat',\n",
        "                  'nyg', 'hehe', 'pen', 'u', 'loh', 'dengan', 'pak', 'bu', 'terus', 'sangat',\n",
        "                  '&amp', 'yah', 'lagi', 'yang', 'kenapa', 'gitu', 'indonesia', 'polisi republik indonesia',\n",
        "                  'kamu', 'saya', 'aku', 'untuk', 'jangan', 'hasil', 'republik', 'polri',\n",
        "                  'dari', 'datang', 'kemana', 'atau', 'lihat', 'sekarang', 'beri',\n",
        "                  'seperti', 'iya', 'kalau', 'pakai', 'dia', 'doang', 'atas', 'tangan',\n",
        "                  'mereka', 'pada', 'dan', 'tapi', 'bilang', 'harus', 'apa', 'saja', 'polisi'\n",
        "                  ]\n",
        "\n",
        "stopword = factory.create_stop_word_remover()\n",
        "stopwords = factory.get_stop_words() + more_stopword"
      ],
      "metadata": {
        "id": "6-L1TEkF8ujY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stopwords)"
      ],
      "metadata": {
        "id": "bviu1Epv8yZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply stopword removal function\n",
        "data['stopwords_tweet'] = data['normalized_tweet'].apply(lambda tweet: [word for word in tweet if word not in stopwords])\n",
        "\n",
        "print(data['stopwords_tweet'])\n",
        "data.head(5)"
      ],
      "metadata": {
        "id": "9q82WJi38zuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.to_csv('df_stopwords_tweet.csv', index=False)\n",
        "data.head(5)"
      ],
      "metadata": {
        "id": "Egx-1HI981Cb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Stemming"
      ],
      "metadata": {
        "id": "koEeCdf783j2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "def stemmed_wrapper(term):\n",
        "    return stemmer.stem(term)\n",
        "\n",
        "term_dict = {}\n",
        "\n",
        "for document in data['stopwords_tweet']:\n",
        "    for term in document:\n",
        "        if term not in term_dict:\n",
        "            term_dict[term] = ' '\n",
        "\n",
        "print(len(term_dict))\n",
        "print(\"------------------------\")\n",
        "\n",
        "for term in term_dict:\n",
        "    term_dict[term] = stemmed_wrapper(term)\n",
        "    print(term,\":\" ,term_dict[term])\n",
        "\n",
        "print(term_dict)\n",
        "print(\"------------------------\")\n",
        "\n",
        "# apply stemmed term to dataframe\n",
        "def get_stemmed_term(document):\n",
        "    return [term_dict[term] for term in document]\n",
        "\n",
        "data['stemmed_tweet'] = data['stopwords_tweet'].swifter.apply(get_stemmed_term)\n",
        "print(data['stemmed_tweet'])"
      ],
      "metadata": {
        "id": "n_sXB3Nq84Zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.to_csv('df_stemmed_tweet.csv', index=False)\n",
        "data.head(5)"
      ],
      "metadata": {
        "id": "jJ1m2wVH8_n4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "factory = StopWordRemoverFactory()\n",
        "more_stopword = ['yg', 'dg', 'rt', 'dgn', 'ny', 'd', 'klo', 'bapak', 'tingkat', 'buat',\n",
        "                  'kalo', 'amp', 'biar', 'bikin', 'bilang','ibu', 'jadi', 'laku',\n",
        "                  'gak', 'ga', 'krn', 'nya', 'nih', 'sih', 'anda', 'terus', 'maksimal',\n",
        "                  'si', 'tau', 'tdk', 'tuh', 'utk', 'ya', 'ente', 'masyarakat',\n",
        "                  'jd', 'jgn', 'sdh', 'aja', 'n', 't', 'dan', 'sangat', 'moga', 'rakyat',\n",
        "                  'nyg', 'hehe', 'pen', 'u', 'loh', 'dengan', 'pak', 'bu', 'terus', 'sangat',\n",
        "                  '&amp', 'yah', 'lagi', 'yang', 'kenapa', 'gitu', 'indonesia', 'polisi republik indonesia',\n",
        "                  'kamu', 'saya', 'aku', 'untuk', 'jangan', 'hasil', 'republik',\n",
        "                  'dari', 'datang', 'kemana', 'atau', 'lihat', 'sekarang', 'beri',\n",
        "                  'seperti', 'iya', 'kalau', 'pakai', 'dia', 'doang', 'atas', 'tangan',\n",
        "                  'mereka', 'pada', 'dan', 'tapi', 'bilang', 'harus', 'apa', 'saja', 'polisi'\n",
        "                  ]\n",
        "\n",
        "stopword = factory.create_stop_word_remover()\n",
        "stopwords = factory.get_stop_words() + more_stopword\n",
        "\n",
        "# Apply stopword removal function\n",
        "data['clean'] = data['stemmed_tweet'].apply(lambda tweet: [word for word in tweet if word not in stopwords])\n",
        "\n",
        "data.to_csv('clean.csv', index=False)"
      ],
      "metadata": {
        "id": "lVVher7m9B2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "factory = StopWordRemoverFactory()\n",
        "stopwords = factory.get_stop_words()\n",
        "print(stopwords)"
      ],
      "metadata": {
        "id": "_5vdJ8Oa9D9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LABELING"
      ],
      "metadata": {
        "id": "mCVLU2419K0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get dataset url from GitHub repository\n",
        "url = \"https://raw.githubusercontent.com/dekapd99/svm-sentiment-analysis/master/stemmed_tweet.csv\"\n",
        "\n",
        "# Read the dataset with pandas\n",
        "try:\n",
        "    data = pd.read_csv(url, delimiter=',')\n",
        "except pd.errors.ParserError as e:\n",
        "    print(\"Error occurred while parsing the CSV file:\", e)\n",
        "\n",
        "# Convert the string representation of the list to a list of words\n",
        "data['stemmed_tweet'] = data['stemmed_tweet'].apply(lambda x: ast.literal_eval(x))\n",
        "\n",
        "# Join the words in 'stemmed_tweet' column into sentences\n",
        "data['sentence'] = data['stemmed_tweet'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Drop the unused Columns\n",
        "data = data.drop(['new_date', 'clean_tweet', 'sentiment'], axis=1)\n",
        "\n",
        "data"
      ],
      "metadata": {
        "id": "YryY0Dn09G8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained sentiment analysis model for Indonesian\n",
        "model_name = \"indobenchmark/indobert-base-p2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "def classify_sentiment(text):\n",
        "    input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
        "    input_ids = input_ids[:1024]  # Truncate to maximum length of 512 tokens\n",
        "\n",
        "    inputs = torch.tensor([input_ids])\n",
        "    outputs = model(inputs)\n",
        "    logits = outputs.logits\n",
        "    predicted_class = logits.argmax(dim=1).item()\n",
        "\n",
        "    sentiment_labels = [\"negatif\", \"positif\"]\n",
        "\n",
        "    if predicted_class < len(sentiment_labels):\n",
        "        sentiment = sentiment_labels[predicted_class]\n",
        "    else:\n",
        "        sentiment = \"Tidak Diketahui\"\n",
        "\n",
        "    return sentiment\n",
        "\n",
        "# Apply sentiment classification to the \"sentence\" column\n",
        "data['sentiment'] = data['sentence'].apply(classify_sentiment)\n",
        "\n",
        "# Display the updated DataFrame with sentiment values\n",
        "print(data[['sentence', 'sentiment']])"
      ],
      "metadata": {
        "id": "MJ7G4uMX96gg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#VISUALISASI DATA"
      ],
      "metadata": {
        "id": "_w73Ouz4-Yi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get dataset url from github repository\n",
        "url = \"https://raw.githubusercontent.com/dekapd99/svm-sentiment-analysis/master/labeling.csv\"\n",
        "\n",
        "# Read the dataset with pandas\n",
        "try:\n",
        "    data = pd.read_csv(url, delimiter=',')\n",
        "except pd.errors.ParserError as e:\n",
        "    print(\"Error occurred while parsing the CSV file:\", e)\n",
        "\n",
        "data"
      ],
      "metadata": {
        "id": "r2k22Uur-aNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Value Count untuk Seluruh Sentiment"
      ],
      "metadata": {
        "id": "RW91SN1Z-piT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_counts = data['sentiment'].value_counts()\n",
        "\n",
        "# Calculate the sum based on the counts\n",
        "total_sum = sentiment_counts.sum()\n",
        "\n",
        "# Display the counts & sum\n",
        "print(sentiment_counts)\n",
        "print(\"Total:\", total_sum)"
      ],
      "metadata": {
        "id": "H7SkohIK-cJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Distribusi Kelas Negatif dan Positif dengan Barchart"
      ],
      "metadata": {
        "id": "eE0FhRd4-zND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the occurrences of each sentiment value\n",
        "sentiment_counts = data['sentiment'].value_counts()\n",
        "\n",
        "# Create the bar chart\n",
        "plt.bar(sentiment_counts.index, sentiment_counts.values)\n",
        "\n",
        "# Set the tick labels for x-axis\n",
        "plt.xticks(sentiment_counts.index, ['Positif', 'Negatif'])\n",
        "\n",
        "# Set the title and labels\n",
        "plt.title(\"Barchart Sentimen\")\n",
        "plt.xlabel(\"Sentiment\")\n",
        "plt.ylabel(\"Count\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "L3sA7K-o-iTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Distribusi Kelas Negatif dan Positif dengan Piechart"
      ],
      "metadata": {
        "id": "jwYk2G_I-7En"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the occurrences of each sentiment value\n",
        "sentiment_counts = data['sentiment'].value_counts()\n",
        "\n",
        "# Create the pie chart\n",
        "plt.pie(sentiment_counts, autopct='%.2f')\n",
        "\n",
        "# Set the title and legend\n",
        "plt.title(\"PieChart Sentiment\")\n",
        "plt.legend(sentiment_counts.index)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TngzQaXo-k5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Word Cloud"
      ],
      "metadata": {
        "id": "L2tXFNpW-91S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "# Convert float values to strings\n",
        "sentence_texts = data['sentence'].astype(str)\n",
        "wc = WordCloud(max_words=1000, min_font_size=10, height=600, width=1200, collocations=False,\n",
        "               background_color='#e6ffed', colormap='summer').generate(' '.join(sentence_texts))\n",
        "\n",
        "plt.axis('off')\n",
        "plt.imshow(wc)"
      ],
      "metadata": {
        "id": "IF7GdiiO-mXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Word Cloud Positif"
      ],
      "metadata": {
        "id": "RKkAr2sz-nA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get dataset url from github repository\n",
        "url = \"https://raw.githubusercontent.com/dekapd99/svm-sentiment-analysis/master/labeling-positif.csv\"\n",
        "\n",
        "# Read the dataset with pandas\n",
        "try:\n",
        "    data_positif = pd.read_csv(url, delimiter=',')\n",
        "except pd.errors.ParserError as e:\n",
        "    print(\"Error occurred while parsing the CSV file:\", e)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "# Convert float values to strings\n",
        "positif_sentence = data_positif['sentence'].astype(str)\n",
        "wc_positif = WordCloud(max_words=1000, min_font_size=10, height=600, width=1200, collocations=True,\n",
        "               background_color='#e6ffed', colormap='summer').generate(' '.join(positif_sentence))\n",
        "\n",
        "plt.axis('off')\n",
        "plt.imshow(wc_positif)\n",
        "\n",
        "# Join all sentences into a single string\n",
        "positif_text = ' '.join(positif_sentence)\n",
        "\n",
        "# Generate word frequency count\n",
        "positif_word_counts = pd.Series(positif_text.split()).value_counts()\n",
        "\n",
        "# Display the word frequency count\n",
        "print(positif_word_counts.head(10))"
      ],
      "metadata": {
        "id": "TyIs7bAE_A7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Word Cloud Negatif"
      ],
      "metadata": {
        "id": "RRjsR6r__IM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get dataset url from github repository\n",
        "url = \"https://raw.githubusercontent.com/dekapd99/svm-sentiment-analysis/master/labeling-negatif.csv\"\n",
        "\n",
        "# Read the dataset with pandas\n",
        "try:\n",
        "    data_negatif = pd.read_csv(url, delimiter=',')\n",
        "except pd.errors.ParserError as e:\n",
        "    print(\"Error occurred while parsing the CSV file:\", e)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "# Convert float values to strings\n",
        "negatif_sentence = data_negatif['sentence'].astype(str)\n",
        "wc_negatif = WordCloud(max_words=1000, min_font_size=10, height=600, width=1200, collocations=True,\n",
        "               background_color='#e6ffed', colormap='autumn').generate(' '.join(negatif_sentence))\n",
        "\n",
        "plt.axis('off')\n",
        "plt.imshow(wc_negatif)\n",
        "\n",
        "# Join all sentences into a single string\n",
        "negatif_text = ' '.join(negatif_sentence)\n",
        "\n",
        "# Generate word frequency count\n",
        "negatif_word_counts = pd.Series(negatif_text.split()).value_counts()\n",
        "\n",
        "# Display the word frequency count\n",
        "print(negatif_word_counts.head(10))"
      ],
      "metadata": {
        "id": "6Rc8recI_KVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3D Visualisasi"
      ],
      "metadata": {
        "id": "-zdTsgNV_R-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat data sentimen positif dan negatif\n",
        "n_samples_positif = 3224\n",
        "n_samples_negatif = 1141\n",
        "\n",
        "# Menghasilkan data sentimen positif\n",
        "mean_positif = [1, 1, 1]\n",
        "cov_positif = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n",
        "positive_samples = np.random.multivariate_normal(mean_positif, cov_positif, n_samples_positif)\n",
        "\n",
        "# Menghasilkan data sentimen negatif\n",
        "mean_negatif = [-1, -1, -1]\n",
        "cov_negatif = [[1, 0, 0], [0, 1, 0], [0, 0, 1]]\n",
        "negative_samples = np.random.multivariate_normal(mean_negatif, cov_negatif, n_samples_negatif)\n",
        "\n",
        "# Membuat subplot 3D\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Plot data sentimen positif\n",
        "ax.scatter(positive_samples[:, 0], positive_samples[:, 1], positive_samples[:, 2], c='blue', label='Positif')\n",
        "\n",
        "# Plot data sentimen negatif\n",
        "ax.scatter(negative_samples[:, 0], negative_samples[:, 1], negative_samples[:, 2], c='red', label='Negatif')\n",
        "\n",
        "# Konfigurasi label sumbu\n",
        "ax.set_xlabel('Dimensi 1')\n",
        "ax.set_ylabel('Dimensi 2')\n",
        "ax.set_zlabel('Dimensi 3')\n",
        "\n",
        "# Tampilkan legenda\n",
        "ax.legend()\n",
        "\n",
        "# Tampilkan plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jzVvxxRB_RcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pembuatan Model dan Pengujian Hasil"
      ],
      "metadata": {
        "id": "53csAc-M_Z1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get dataset url from github repository\n",
        "url = \"https://raw.githubusercontent.com/dekapd99/svm-sentiment-analysis/master/labeling.csv\"\n",
        "\n",
        "# Read the dataset with pandas\n",
        "try:\n",
        "    data = pd.read_csv(url, delimiter=',')\n",
        "except pd.errors.ParserError as e:\n",
        "    print(\"Error occurred while parsing the CSV file:\", e)\n",
        "\n",
        "data"
      ],
      "metadata": {
        "id": "3__qCrLk_ewU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Oversampling Data dengan Duplikasi"
      ],
      "metadata": {
        "id": "YTWzsTEjAemk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengekstrak sampel negatif dan positif\n",
        "negative_samples = data[data['sentiment'] == 'negatif']\n",
        "positive_samples = data[data['sentiment'] == 'positif']\n",
        "\n",
        "# Oversampling sampel negatif\n",
        "oversampled_negative = resample(negative_samples,\n",
        "                                replace=True,  # Mengizinkan duplikasi\n",
        "                                n_samples=len(positive_samples),  # Jumlah sampel yang sama dengan sampel positif\n",
        "                                random_state=42)\n",
        "\n",
        "# Menggabungkan kembali sampel positif dengan sampel negatif yang sudah di-oversampling\n",
        "oversampled_data = pd.concat([positive_samples, oversampled_negative])\n",
        "\n",
        "# Count the number of samples in the \"negatif\" and \"positif\" classes\n",
        "count_negatif = len(oversampled_data[oversampled_data['sentiment'] == 'negatif'])\n",
        "count_positif = len(oversampled_data[oversampled_data['sentiment'] == 'positif'])\n",
        "\n",
        "# Print the counts\n",
        "print(\"Jumlah sampel negatif:\", count_negatif)\n",
        "print(\"Jumlah sampel positif:\", count_positif)"
      ],
      "metadata": {
        "id": "1hkGwCTu_woH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the occurrences of each sentiment value\n",
        "sentiment_counts = oversampled_data['sentiment'].value_counts()\n",
        "\n",
        "# Create the bar chart\n",
        "plt.bar(sentiment_counts.index, sentiment_counts.values)\n",
        "\n",
        "# Set the tick labels for x-axis\n",
        "plt.xticks(sentiment_counts.index, ['Positif', 'Negatif'])\n",
        "\n",
        "# Set the title and labels\n",
        "plt.title(\"Barchart Sentimen\")\n",
        "plt.xlabel(\"Sentiment\")\n",
        "plt.ylabel(\"Count\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Qsv7-kB__zDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TF-IDF Vectorizer"
      ],
      "metadata": {
        "id": "TNra-J9qAlPc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create TF-IDF vectorizer\n",
        "oversampled_data.dropna(subset=['sentence'], inplace=True)\n",
        "\n",
        "tfidfconverter = TfidfVectorizer(max_features=None,\n",
        "                                 ngram_range=(1,2), analyzer=\"word\", min_df=0, max_df=0.9,\n",
        "                                 smooth_idf=True, use_idf=True)\n",
        "X = tfidfconverter.fit_transform(oversampled_data['sentence']).toarray()\n",
        "\n",
        "print(X)"
      ],
      "metadata": {
        "id": "q31slrGC_1ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Skenario Pengujian 60:40"
      ],
      "metadata": {
        "id": "lURC7GxvAaG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, oversampled_data['sentiment'],\n",
        "                                                    test_size=0.40, random_state=42)"
      ],
      "metadata": {
        "id": "QP_LbgRD_4J7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X_train count:\", X_train.shape[0])\n",
        "print(\"X_test count:\", X_test.shape[0])\n",
        "print(\"y_train count:\", y_train.shape[0])\n",
        "print(\"y_test count:\", y_test.shape[0])"
      ],
      "metadata": {
        "id": "TVhKcdzq_6qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_svm = SVC(kernel='rbf', random_state=42)\n",
        "model_svm.fit(X_train, y_train)\n",
        "predicted_svm = model_svm.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, predicted_svm)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, predicted_svm))\n",
        "\n",
        "accuracy = accuracy_score(y_test, predicted_svm)\n",
        "print(\"Accuracy:\", accuracy * 100)"
      ],
      "metadata": {
        "id": "eMjcWgkKAFFB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "confusionmatrix = confusion_matrix(y_test, predicted_svm)\n",
        "f, ax = plt.subplots(figsize =(5,5))\n",
        "sns.heatmap(confusionmatrix,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\n",
        "plt.xlabel(\"predicted_svm\")\n",
        "plt.ylabel(\"y_test\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KSIp5oI4AZOE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Skenario Pengujian 70:30"
      ],
      "metadata": {
        "id": "GkVrAYPxA6ho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, oversampled_data['sentiment'],\n",
        "                                                    test_size=0.30, random_state=42)"
      ],
      "metadata": {
        "id": "RfeOulidA_dQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X_train count:\", X_train.shape[0])\n",
        "print(\"X_test count:\", X_test.shape[0])\n",
        "print(\"y_train count:\", y_train.shape[0])\n",
        "print(\"y_test count:\", y_test.shape[0])"
      ],
      "metadata": {
        "id": "_aoclUt9BC_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_svm = SVC(kernel='rbf', random_state=42)\n",
        "model_svm.fit(X_train, y_train)\n",
        "predicted_svm = model_svm.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, predicted_svm)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, predicted_svm))\n",
        "\n",
        "accuracy = accuracy_score(y_test, predicted_svm)\n",
        "print(\"Accuracy:\", accuracy * 100)"
      ],
      "metadata": {
        "id": "4zgg_-xxBFWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "confusionmatrix = confusion_matrix(y_test, predicted_svm)\n",
        "f, ax = plt.subplots(figsize =(5,5))\n",
        "sns.heatmap(confusionmatrix,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\n",
        "plt.xlabel(\"predicted_svm\")\n",
        "plt.ylabel(\"y_test\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dvALkqu_BGWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Skenario Pengujian 80:20"
      ],
      "metadata": {
        "id": "vPMH9xW4BHP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, oversampled_data['sentiment'],\n",
        "                                                    test_size=0.30, random_state=42)"
      ],
      "metadata": {
        "id": "bWWtjD7PBIzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"X_train count:\", X_train.shape[0])\n",
        "print(\"X_test count:\", X_test.shape[0])\n",
        "print(\"y_train count:\", y_train.shape[0])\n",
        "print(\"y_test count:\", y_test.shape[0])"
      ],
      "metadata": {
        "id": "KE1NmMvaBLYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_svm = SVC(kernel='rbf', random_state=42)\n",
        "model_svm.fit(X_train, y_train)\n",
        "predicted_svm = model_svm.predict(X_test)\n",
        "\n",
        "cm = confusion_matrix(y_test, predicted_svm)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, predicted_svm))\n",
        "\n",
        "accuracy = accuracy_score(y_test, predicted_svm)\n",
        "print(\"Accuracy:\", accuracy * 100)"
      ],
      "metadata": {
        "id": "D--Hcd2-BMTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "confusionmatrix = confusion_matrix(y_test, predicted_svm)\n",
        "f, ax = plt.subplots(figsize =(5,5))\n",
        "sns.heatmap(confusionmatrix,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\n",
        "plt.xlabel(\"predicted_svm\")\n",
        "plt.ylabel(\"y_test\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uNdFclTbBNZM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}